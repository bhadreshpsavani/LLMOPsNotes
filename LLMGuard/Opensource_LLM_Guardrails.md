# LLMGuard

## Opensource Frameworks and Libraries

### [guardrails-ai](https://github.com/guardrails-ai/guardrails)
Guardrails is a Python framework that helps build reliable AI applications by performing two key functions:
1) Guardrails runs Input/Output Guards in your application that detect, quantify and mitigate the presence of specific types of risks. To look at the full suite of risks, check out Guardrails Hub.
2) Guardrails help you generate structured data from LLMs.
![with_and_without_guardrails](https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/img/with_and_without_guardrails.svg)

### [protectai-llm-guard](https://github.com/protectai/llm-guard)
offering sanitization, detection of harmful language, prevention of data leakage, and resistance against prompt injection attacks, LLM-Guard ensures that your interactions with LLMs remain safe and secure.
![protectai-llm-guard](https://github.com/protectai/llm-guard/blob/main/docs/assets/flow.png?raw=true)

### [NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)
NeMo Guardrails enables developers building LLM-based applications to easily add programmable guardrails between the application code and the LLM.
![programmable_guardrails](https://github.com/NVIDIA/NeMo-Guardrails/raw/develop/docs/_static/images/programmable_guardrails.png)

## Links
* [build-safe-and-responsible-generative-ai-applications-with-guardrails-aws](https://aws.amazon.com/blogs/machine-learning/build-safe-and-responsible-generative-ai-applications-with-guardrails/)
